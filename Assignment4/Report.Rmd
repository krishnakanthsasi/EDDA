---
title: "Assignment 4"
author: "Tommy Maaiveld, Krishnakanth Sasi, Halil Kaan Kara, Group 6"
output: pdf_document
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = TRUE)

library(lme4)
```


## Introduction

## Question 1

### Section 1
```{r, fig.align='center'}
fruitflies = read.table(file="data/fruitflies.txt", header=TRUE)
fruitflies = cbind(fruitflies, log(fruitflies[,2]))
names(fruitflies)[4] = "loglongevity"
head(fruitflies,6) # some output deleted
```

### Section 2
```{r, fig.align='center'}
attach(fruitflies)
plot(loglongevity~thorax,pch=as.character(activity))
```

The plot shows a linear correlation between thoractic length (`thorax`) and log longevity. It seems to indicate that flies with the `activity` factor set to `high` live shorter than those with `low`, which in turn score lower than those with `isolated`, assuming equal thoractic length between specimens. 

### Section 3

```{r, fig.align='center'}
fruitfliesaov = lm(loglongevity~activity, data=fruitflies)
attach(fruitfliesaov)
anova(fruitfliesaov)
```

According to this analysis, `activity` seems likely to have an effect on `loglongevity`, since the p-value $< 0.05$ (p-value $\approx`r round(anova(fruitfliesaov)$"Pr(>F)"[1],9)`$). Thus, sexual activity seems to influence longevity.

### Section 4
the analysis shows that sexual activity decreases longevity in fruitflies, since the `activity` factor affects `loglongevity`. 

```{r, fig.align='center'}
summary(fruitfliesaov)
```

#  could recheck these means by doing visual to check if they are valid... Not sure if I interpreted estimates right from the slide

the longevity estimates for each level of the factor `activity` are `r fruitfliesaov$coef[1]` for fruitflies of level `high`, `r fruitfliesaov$coef[1]+fruitfliesaov$coef[3]` for fruitflies of level `low`, and `r fruitfliesaov$coef[1] + fruitfliesaov$coef[2]` for fruitflies of level `isolated`. As sexual activity increases, lifespan decreases.

### Section 5

```{r, fig.align='center'}
fruitfliesfullaov = lm(loglongevity~thorax+activity, data=fruitflies)
attach(fruitfliesfullaov)
anova(fruitfliesfullaov)
summary(fruitfliesfullaov)$coef

```

The output gives the following rounded estimates for the model coefficients $\mu,\,\beta\,\alpha_{1}$ and $\alpha_{2}$:  
$\mu = `r round(fruitfliesfullaov$coef[1],3)`$   
$\beta =  `r round(fruitfliesfullaov$coef[2],3)`$  
$\alpha_{1} = `r round(fruitfliesfullaov$coef[4],3)`$  
$\alpha_{2} = `r round(fruitfliesfullaov$coef[3],3)`$  
Where mu is the estimate for an average fly with high sexual activity, beta is the parameters The p-values are all virtually zero, meaning there is almost no risk of a type I error. Sexual activity is very likely to influence longevity, regardless of whether thorax length is taken into account.

### Section 6
Sexual activity decreases longevity, since the estimate is lowest for flies with high sexual activity, and highest for isolated flies. 

```{r, fig.align='center'}
mean(fruitflies[,1])
min(fruitflies[,1])
```

For an average fly with a thorax length $\approx `r round(mean(fruitflies[,1]),2)`,$ the value for parameter $\beta$ given above can be used in a sum to compute all three estimates for loglongevity based on variable $X_{avg}$:  
  
$\mu + \beta * X_{avg} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r round(mean(fruitflies[,1]),2)` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*mean(fruitflies[,1])),3)`$ (high sexual activity)  

$\mu + \beta * X_{avg} + \alpha_{1} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r round(mean(fruitflies[,1]),2)` + `r round(fruitfliesfullaov$coef[4],3)` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*mean(fruitflies[,1])+fruitfliesfullaov$coef[4]),3)`$ (low sexual activity)  

$\mu + \beta * X_{avg} + \alpha_{2} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r round(mean(fruitflies[,1]),2)` + `r round(fruitfliesfullaov$coef[3],3)` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*mean(fruitflies[,1])+fruitfliesfullaov$coef[3]),3)`$ (isolated specimen) 

In order to compute the estimates for a fly as small as the smallest fly in the dataset, the term $X$ is substituted with $X_{min}$, the thorax size of the smallest fly in the dataset (`r min(fruitflies[,1])`).

$\mu + \beta * X_{min} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r min(fruitflies[,1])` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*min(fruitflies[,1])),3)`$ (high sexual activity)  

$\mu + \beta * X_{min} + \alpha_{1} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r min(fruitflies[,1])` + `r round(fruitfliesfullaov$coef[4],3)` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*min(fruitflies[,1])+fruitfliesfullaov$coef[4]),3)`$ (low sexual activity)  

$\mu + \beta * X_{min} + \alpha_{2} = `r round(fruitfliesfullaov$coef[1],3)` + `r round(fruitfliesfullaov$coef[2],3)` * `r min(fruitflies[,1])` + `r round(fruitfliesfullaov$coef[3],3)` = `r round((fruitfliesfullaov$coef[1]+fruitfliesfullaov$coef[2]*min(fruitflies[,1])+fruitfliesfullaov$coef[3]),3)`$ (isolated specimen) 

### Section 7

```{r, fig.align='center'}
plot(loglongevity~thorax,pch=unclass(activity))
for (i in c("high", "low", "isolated")) abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity==i,]))
```

The given plot shows fit lines for each level of the `activity` factor. Thorax length correlates positively with longevity (bigger flies live longer), meaning $\beta$ is expected to be nonzero. The fit lines in the plot converge slightly, although the true lines could still be parallel. In other words, the parameter $\beta$ is similar for each factor level, meaning the dependence on thorax length is similar on each level.
This means that the lifespan of a given fruitfly is affected by its sexual activity, regardless of its size, and bigger flies live longer within each factor level. The slight convergence could be interpreted as bigger flies being more 'resistant' to the detrimental effect of sexual activity on longevity, since there is a more pronounced difference in `loglongevity` for small flies compared to large ones visible in the plot.

### Section 8

In the isolation group, the flies have little reason to compete, since there are no limited resources to compete over. However, when a limited 

## Question 2

### Section 1

Given data set for this question consists of one binary response and two explanatory variables which one of them is also a binary variable. Scatter plot of passed and gpa can be seen below. It can be seen that a linear line cannot be fitted to this data set.

```{r, fig.align='center'}
psiData = read.table("./data/psi.txt", header = TRUE)
psiDataNonFactor = data.frame(psiData)

psiData$passed = ifelse(test=psiData$passed == 1, yes="Pass", no="Fail")
psiData$passed = as.factor(psiData$passed)

psiData$psi = ifelse(test=psiData$psi == 1, yes="Yes", no="No")
psiData$psi = as.factor(psiData$psi)

par(mfrow=c(1,1))
plot(passed ~ gpa, data = psiDataNonFactor)
```

The numeric variable; gpa seems to be from a standart normal distribution and its histogram and QQ-Plot can be seen below. As a first step, binary variables are converted into factors.

```{r, fig.align='center'}
par(mfrow=c(1,2))
hist(psiData$gpa, freq = FALSE)
qqnorm(psiData$gpa)
```

Table of combination of two binary variables can be seen in the table below. From this table we can say that psi is looking promising since more students have passed upon receiving psi.

```{r}

str(psiData)

xtabs(~passed + psi, data = psiData)

```

### Section 2

The output of the basic logistic regression model fitted with glm command using both numeric and binary variables can be seen below. The model is trained on training data set and validated on test data set as can be seen below. The test data set uses 10% of the whole data set without replacement.

```{r}
# Fit the model
logRegModel = glm(passed ~ psi + gpa, data = psiData, family = "binomial")
logSummary = summary(logRegModel)
logSummary
```

From the output, this model corresponds to the equation given below.

$P(Y) = \Psi(`r logSummary$coefficients[1, 1]` + (`r logSummary$coefficients[2, 1]`) * psi + (`r logSummary$coefficients[3, 1]`) * gpa )$

Graph of this equations predictions can be seen below.

```{r, fig.align='center'}
newdat1 = data.frame(gpa=seq(0, 4, len=300))
newdat2 = data.frame(gpa=seq(0, 4, len=300))

newdat1$psi = 1
newdat1$psi = ifelse(test=newdat1$psi == 1, yes="Yes", no="No")
newdat1$psi = as.factor(newdat1$psi)
newdat2$psi = 0
newdat2$psi = ifelse(test=newdat2$psi == 1, yes="Yes", no="No")
newdat2$psi = as.factor(newdat2$psi)

newdat1$passed = predict(logRegModel, newdata=newdat1, type="response")
newdat2$passed = predict(logRegModel, newdata=newdat2, type="response")

par(mfrow=c(1,1))
plot(passed ~ gpa, data = psiDataNonFactor, xlim = c(1.5 ,4))
lines(passed ~ gpa, data = newdat1, col="green4", lwd=2)
lines(passed ~ gpa, data = newdat2, col="red4", lwd=2)
legend(x = "left", legend=c("PSI = YES", "PSI = NO"),
       col=c("green", "red"), lty=1:1, cex=0.8)
```

### Section 3

From the table given in Section 1, we can calculate the probability of a student passing the assignment given he or she received psi is $P(Passed=TRUE | PSI=TRUE) = `r (8/32)/(14/32)`$. From the predictions made with the model given in Section 2, we see higher probabilities for students which received psi. Also, the graph for logistic curve given in Section 2 clearly demonstrates the effect of psi in a positive way. Finally, we can check the coefficient of the equation for psi which is `r logSummary$coefficients[2, 1]`. All these information point that psi works.

### Section 4

Probabilities for one student having gpa of 3 and receiving psi and one student having gpa of 3 and not receiving  psi is given below. 

```{r}
testSec4 = read.table("./data/psi-section4.txt", header = TRUE)
testSec4$passed = ifelse(test=testSec4$passed == 1, yes="Pass", no="Fail")
testSec4$passed = as.factor(testSec4$passed)
testSec4$psi = ifelse(test=testSec4$psi == 1, yes="Yes", no="No")
testSec4$psi = as.factor(testSec4$psi)

testSec4 # Passed column is irrelevant in this case

predicted = predict(logRegModel, testSec4, type = "response")
predicted
```

### Section 5

Estimation of relative change in odds can be seen below with the command. This command yields two numbers for each explanatory variable.

```{r}
odds = round(exp(logRegModel$coefficients), 3)
odds
```

From the output of the command, we can say that if the student has received psi, the odds of that student passing increase by a factor of `r odds[2]`. So it can be said that psi works better than the standard teaching method since odds of a student increases upon receiving psi regardless of the student's gpa. Also, we can say for a one unit increase in gpa of a student, the odds of that student passing increase by a factor of `r odds[3]` of the teaching method.

### Section 6

The table of the alternate analysis can be seen below. This table looks familiar with the table shown in Section 1. With the table from Section 1 in mind, we can assume numbers 15 and 6 are the students failed regardles of the teaching method. So this is a table which is showing the combinations of the binary response variable and the binary explanatory variable.

```{r}
x = matrix(c(3, 15, 8, 6), 2, 2)
x
fisher.test(x)
```

The outcome of Fisher's Exact Test with the p-value of 0.0265, describes that the difference of the probabilities of the matrix are statistically significant. This means that the proportions at one variable are not the same for different values of the second variable thus the variables are not independent of each other.

### Section 7

Fisher's Exact Test yields whether the proportions of two nominal variables are different depending on the value of the other variable. So it is a test of independency. This test is not appropriate for this case since we are after the probabilities of students passing or failing based on the teaching method and gpa.

### Section 8

Fisher's Exact Test

    - Advantage: Identifies the significance of a relationship.
    - Disadvantage: It fits for small data sets but computationally expensive for large data sets.

Logistic Regression
    
    - Advantage: Constructs a model that measure the relationship between the dependent variable and the independent variables by estimating the probabilities using logistic function.
    - Disadvantage: Logistic Regression will not work if there is a feature that completely separates the two classes.

## Question 3

### Section 1


```{r, fig.align='center'}
par(mfrow=c(1,3))
hist(rpois(10000,.1), cex.main=.8); hist(rpois(10000,.5)); hist(rpois(10000,1))
hist(rpois(10000,5)); hist(rpois(10000,10)); hist(rpois(10000,100))
hist(rpois(10,1000)); hist(rpois(100,1000)); hist(rpois(1000,10000))
```
For larger values of $\lambda$, the distribution is similar to a normal distribution with the mean and variance both equal to $\lambda$. Parameter $n$ is of limited influence - it merely determines the amount of values to be sampled from the Poisson distribution. So long as a reasonable amount of points are sampled, the same distribution should emerge for equal $\lambda$.

### Section 2

In order for the distribution of a randomly distributed variable $Y$ to be in a location-scale family as a given random variable $X$, $Y$ must have the same distribution as $a + bX$ for some parameters $a$ and $b$ (in other words, $Y \stackrel{d}{=} a + b X$, where $Y \stackrel{d}{=}$ means 'equal in distribution'. 

In the case of the Poisson distribution, the distribution is both scaled by parameter $\lambda$, since the mean and variance are both equal to $\lambda$. Thus, it can be said that, given a variable $Y$ and a variable $X$ that follow a Poisson distribution, $Y \stackrel{d}{=} \lambda X$, which satisfies the above condition for location-scale families.

However, for very small values of lambda ($\lambda < 1$),where the distribution looks less similar to a normal distribution, it may prove difficult to produce Poisson distributions with larger $\lambda$ values via a linear transformation, as a scaling transformation may not be able to fit a normal distribution.

### Section 3

```{r, fig.align='center', fig.height=6}
africa = read.table("data/africa.txt",header=TRUE)
africaglm=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
              family=poisson,data=africa)

plot(africa)

summary(africaglm)
confint(africaglm)
coef(africaglm)
```

```{r, fig.align='center', fig.height=2.6}
# Assumption checks:
par(mfrow=c(1,3))
plot(fitted(africaglm),residuals(africaglm), main='fitted values vs. residuals')
plot(log(fitted(africaglm)),residuals(africaglm), main='logarithm of fitted values \nvs. residuals')
plot(fitted(africaglm),residuals(africaglm,type="response"), main='fitted values vs. response \n residuals', ylab='response residuals')
```

Performing visual checks on the residuals of the model shows some odd relationships between the relationships and the fitted values, as the variance of the residuals doesn't seem to increase for higher fitted values. This is expected under a Poisson distribution, as higher fitted values correspond to higher variances as lambda is modeled differently for each observation. The first plot also shows some collinearity between variables such as `popn` and `pollib`.

### Section 4

```{r, fig.align='center'}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
            family=poisson,data=africa))
# `numelec` has the highest p-value, and is removed.
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim,
            family=poisson,data=africa))
# `numregim` is removed next.
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size,
            family=poisson,data=africa))
# removing `size`
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn,
            family=poisson,data=africa))
# removing `popn`
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote,
            family=poisson,data=africa))
# removing `pctvote`
summary(glm(miltcoup~oligarchy+pollib+parties,
            family=poisson,data=africa))

```

The remaining parameters appear significant, as their p-value is lower than 0.05. By examining the collinearity of the remaining variables using the plot below, it appears that none of the remaining variables are excessively collinear.

```{r, fig.align='center'}
plot(africa[,1:4])
```