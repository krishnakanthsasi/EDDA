---
title: "Assignment 4"
author: "Tommy Maaiveld, Krishnakanth Sasi, Halil Kaan Kara, Group 6"
output: pdf_document
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = TRUE)

library(lme4)
```


## Introduction
This document describes the solutions found and implemented for the exercises of assignment 3. Exercises can be found in their corresponding sections. This document is created by Rmd, and figure captions are omitted since it changes the structure of the document in a bad way that makes it hard to follow

## Question 1

## Question 2

### Section 1

Given data set for this question consists of one binary response and two explanatory variables which one of them is also a binary variable. Scatter plot of passed and gpa can be seen below. It can be seen that a linear line cannot be fitted to this data set.

```{r, fig.align='center'}
psiData = read.table("./data/psi.txt", header = TRUE)
psiDataNonFactor = data.frame(psiData)

psiData$passed = ifelse(test=psiData$passed == 1, yes="Pass", no="Fail")
psiData$passed = as.factor(psiData$passed)

psiData$psi = ifelse(test=psiData$psi == 1, yes="Yes", no="No")
psiData$psi = as.factor(psiData$psi)

par(mfrow=c(1,1))
plot(passed ~ gpa, data = psiDataNonFactor)
```

The numeric variable; gpa seems to be from a standart normal distribution and its histogram and QQ-Plot can be seen below. As a first step, binary variables are converted into factors.

```{r, fig.align='center'}
par(mfrow=c(1,2))
hist(psiData$gpa, freq = FALSE)
qqnorm(psiData$gpa)
```

Table of combination of two binary variables can be seen in the table below. From this table we can say that psi is looking promising since more students have passed upon receiving psi.

```{r}

str(psiData)

xtabs(~passed + psi, data = psiData)

```

### Section 2

The output of the basic logistic regression model fitted with glm command using both numeric and binary variables can be seen below. The model is trained on training data set and validated on test data set as can be seen below. The test data set uses 10% of the whole data set without replacement.

```{r}
# Fit the model
logRegModel = glm(passed ~ psi + gpa, data = psiData, family = "binomial")
logSummary = summary(logRegModel)
logSummary
```

From the output, this model corresponds to the equation given below.

$P(Y) = \Psi(`r logSummary$coefficients[1, 1]` + (`r logSummary$coefficients[2, 1]`) * psi + (`r logSummary$coefficients[3, 1]`) * gpa )$

Graph of this equations predictions can be seen below.

```{r, fig.align='center'}
newdat1 = data.frame(gpa=seq(0, 4, len=300))
newdat2 = data.frame(gpa=seq(0, 4, len=300))

newdat1$psi = 1
newdat1$psi = ifelse(test=newdat1$psi == 1, yes="Yes", no="No")
newdat1$psi = as.factor(newdat1$psi)
newdat2$psi = 0
newdat2$psi = ifelse(test=newdat2$psi == 1, yes="Yes", no="No")
newdat2$psi = as.factor(newdat2$psi)

newdat1$passed = predict(logRegModel, newdata=newdat1, type="response")
newdat2$passed = predict(logRegModel, newdata=newdat2, type="response")

par(mfrow=c(1,1))
plot(passed ~ gpa, data = psiDataNonFactor, xlim = c(1.5 ,4))
lines(passed ~ gpa, data = newdat1, col="green4", lwd=2)
lines(passed ~ gpa, data = newdat2, col="red4", lwd=2)
legend(x = "left", legend=c("PSI = YES", "PSI = NO"),
       col=c("green", "red"), lty=1:1, cex=0.8)
```

### Section 3

From the table given in Section 1, we can calculate the probability of a student passing the assignment given he or she received psi is $P(Passed=TRUE | PSI=TRUE) = `r (8/32)/(14/32)`$. From the predictions made with the model given in Section 2, we see higher probabilities for students which received psi. Also, the graph for logistic curve given in Section 2 clearly demonstrates the effect of psi in a positive way. Finally, we can check the coefficient of the equation for psi which is `r logSummary$coefficients[2, 1]`. All these information point that psi works.

### Section 4

Probabilities for one student having gpa of 3 and receiving psi and one student having gpa of 3 and not receiving  psi is given below. 

```{r}
testSec4 = read.table("./data/psi-section4.txt", header = TRUE)
testSec4$passed = ifelse(test=testSec4$passed == 1, yes="Pass", no="Fail")
testSec4$passed = as.factor(testSec4$passed)
testSec4$psi = ifelse(test=testSec4$psi == 1, yes="Yes", no="No")
testSec4$psi = as.factor(testSec4$psi)

testSec4 # Passed column is irrelevant in this case

predicted = predict(logRegModel, testSec4, type = "response")
predicted
```

### Section 5

Estimation of relative change in odds can be seen below with the command. This command yields two numbers for each explanatory variable.

```{r}
odds = round(exp(logRegModel$coefficients), 3)
odds
```

From the output of the command, we can say that if the student has received psi, the odds of that student passing increase by a factor of `r odds[2]`. So it can be said that psi works better than the standard teaching method since odds of a student increases upon receiving psi regardless of the student's gpa. Also, we can say for a one unit increase in gpa of a student, the odds of that student passing increase by a factor of `r odds[3]` of the teaching method.

### Section 6

### Section 7

### Section 8

## Question 3

### Section 1


```{r, fig.align='center'}
par(mfrow=c(1,3))
hist(rpois(10000,.1), cex.main=.8); hist(rpois(10000,.5)); hist(rpois(10000,1))
hist(rpois(10000,5)); hist(rpois(10000,10)); hist(rpois(10000,100))
hist(rpois(10,1000)); hist(rpois(100,1000)); hist(rpois(1000,10000))
```
For larger values of $\lambda$, the distribution is similar to a normal distribution with the mean and variance both equal to $\lambda$. Parameter $n$ is of limited influence - it merely determines the amount of values to be sampled from the Poisson distribution. So long as a reasonable amount of points are sampled, the same distribution should emerge for equal $\lambda$.

### Section 2

In order for the distribution of a randomly distributed variable $Y$ to be in a location-scale family as a given random variable $X$, $Y$ must have the same distribution as $a + bX$ for some parameters $a$ and $b$ (in other words, $Y \stackrel{d}{=} a + b X$, where $Y \stackrel{d}{=}$ means 'equal in distribution'. 

In the case of the Poisson distribution, the distribution is both scaled by parameter $\lambda$, since the mean and variance are both equal to $\lambda$. Thus, it can be said that, given a variable $Y$ and a variable $X$ that follow a Poisson distribution, $Y \stackrel{d}{=} \lambda X$, which satisfies the above condition for location-scale families.

However, for very small values of lambda ($\lambda < 1$),where the distribution looks less similar to a normal distribution, it may prove difficult to produce Poisson distributions with larger $\lambda$ values via a linear transformation, as a scaling transformation may not be able to fit a normal distribution.

### Section 3

```{r, fig.align='center', fig.height=6}
africa = read.table("data/africa.txt",header=TRUE)
africaglm=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
              family=poisson,data=africa)

plot(africa)

summary(africaglm)
confint(africaglm)
coef(africaglm)
```

```{r, fig.align='center', fig.height=2.6}
# Assumption checks:
par(mfrow=c(1,3))
plot(fitted(africaglm),residuals(africaglm), main='fitted values vs. residuals')
plot(log(fitted(africaglm)),residuals(africaglm), main='logarithm of fitted values \nvs. residuals')
plot(fitted(africaglm),residuals(africaglm,type="response"), main='fitted values vs. response \n residuals', ylab='response residuals')
```

Performing visual checks on the residuals of the model shows some odd relationships between the relationships and the fitted values, as the variance of the residuals doesn't seem to increase for higher fitted values. This is expected under a Poisson distribution, as higher fitted values correspond to higher variances as lambda is modeled differently for each observation. The first plot also shows some collinearity between variables such as `popn` and `pollib`.

### Section 4

```{r, fig.align='center'}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
            family=poisson,data=africa))
# `numelec` has the highest p-value, and is removed.
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim,
            family=poisson,data=africa))
# `numregim` is removed next.
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size,
            family=poisson,data=africa))
# removing `size`
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn,
            family=poisson,data=africa))
# removing `popn`
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote,
            family=poisson,data=africa))
# removing `pctvote`
summary(glm(miltcoup~oligarchy+pollib+parties,
            family=poisson,data=africa))

```

The remaining parameters appear significant, as their p-value is lower than 0.05. By examining the collinearity of the remaining variables using the plot below, it appears that none of the remaining variables are excessively collinear.

```{r, fig.align='center'}
plot(africa[,1:4])
```